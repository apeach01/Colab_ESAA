{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4H6louPjJ5MqoUyC9kHcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackie-Gung/Colab_ESAA/blob/main/2022_09_23_%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chapter 7. 앙상블 학습과 랜덤 포레스트**\n",
        "---\n"
      ],
      "metadata": {
        "id": "UoGotGIWedYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 일련의 예측기(분류나 회귀모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있음\n",
        "- 이 일련의 예측기를 **앙상블**이라 부르고, 이를 **앙상블 학습**이라고 하며, 앙상블 학습 알고리즘을 **앙상블 방법**이라고 함\n",
        "- 앙상블 방법의 예\n",
        "      1. 훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정트리 분류기를 훈련시킴\n",
        "      2. 예측을 하려면 모든 개별 트리의 예측을 구하면 됨\n",
        "      3. 가장 많은 선택을 받은 클래스를 예측으로 삼음\n",
        "      4. 이 결정 트리의 앙상블을 '랜덤 포레스트'라고 함"
      ],
      "metadata": {
        "id": "GRJsicaGeio1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1 투표 기반 분류기**\n",
        "---"
      ],
      "metadata": {
        "id": "QC5gbr6SpGXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. 직접 투표 분류기:** 여러 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것으로, 다수결 투표로 정해지는 분류기\n",
        "- 다수결 투표 분류기가 앙상블에 포함된 개별 분류기 중 가장 뛰어난 것보다 정확도가 높은 경우가 많음\n",
        "- 각 분류기가 약한 학습기일지라도 충분하고 많다면 앙상블은 강한 학습기가 될 수 있음"
      ],
      "metadata": {
        "id": "_1PEj_C6pJHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# library 불러오기\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_eFacBuNHDJ4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data 불러오기\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train,X_test,y_train,y_test, = train_test_split(X,y,random_state=42)"
      ],
      "metadata": {
        "id": "YL9CKWuEHQv0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9lfcMK5FWTQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dc6323-805e-465d-f094-c39e29cfdd88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
              "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "# 투표 기반 분류기 생성하기\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators = [('lr',log_clf), ('rf',rnd_clf), ('svc',svm_clf)],\n",
        "    voting='hard')\n",
        "\n",
        "# 분류 모델 학습하기\n",
        "voting_clf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf,rnd_clf,voting_clf):\n",
        "  clf.fit(X_train,y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COWfu752Gr7C",
        "outputId": "b2479054-de31-4b09-cd33-3e3a40b9d3c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.864\n",
            "RandomForestClassifier 0.88\n",
            "VotingClassifier 0.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과\n",
        "  - 예상대로 투표 기반 분류기가 다른 개별 분류기보다 성능이 조금 더 높음"
      ],
      "metadata": {
        "id": "mdmwsS2bK_lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. 간접 투표:** 모든 분류기가 클래스의 확률을 예측할 수 있으면(=predict_proba()), 개별 분류기의 예측 평균을 내어 확률이 가장 높은 클래스를 예측할 수 있음\n",
        "##### 1) 특징\n",
        "      1. 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높음\n",
        "      2. 이 방식을 사용하기 위해 voting='hard'를 voting='soft'로 바꾸고 모든 분류기가 클래스의 확률을 추정할 수 있으면 됨\n",
        "      3. SVC는 기본값에서 클래스 확률을 제공하지 않으므로 probability 매개변수를 True로 지정해야함\n",
        "        -> 클래스 확률을 추정하기 위해 교차 검증을 사용하므로 훈련 속도가 느려지지만 predict_proba() 메서드 사용이 가능해짐"
      ],
      "metadata": {
        "id": "aKkqaEIHLUOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.2 배깅과 페이스팅**\n",
        "---"
      ],
      "metadata": {
        "id": "1ZjK79FCMxau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다양한 분류기를 만드는 방법\n",
        "      1. 각기 다른 훈련 알고리즘을 사용하는것\n",
        "      2. 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각각 다르게 학습시키는 것\n",
        "        - 배깅: 훈련 세트에서 중복허용하여 샘플링하는 방식\n",
        "        - 페이스팅: 중복을 허용하지 않고 샘플링하는 방식\n",
        "\n",
        "- 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만듦\n",
        "- 수집 함수는 전형적으로 분류일 때 **통계적 최빈값**, 회귀에는 **평균**을 계산함\n",
        "- 개별 예측기는 원본 훈련 세트로 훈련시킨 것보다 훨씬 더 크게 편향되어 있으나 수집 함수를 통과하면 편향과 분산이 모두 감소함\n",
        "- 일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 줄어듦"
      ],
      "metadata": {
        "id": "IAoz1ZP2M3Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **7.2.1 사이킷런의 배깅과 페이스팅**"
      ],
      "metadata": {
        "id": "aI8l6c4BPWrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결정 트리 분류기 500개의 앙상블 훈련\n",
        "- 각 분류기는 훈련 세트에서 중복허용하여 무작위로 선택된 100개의 샘플로 훈련됨\n",
        "  - 이는 배깅에 해당됨(페이스팅은 bootstrap=False 지정)\n",
        "\n",
        "- n_jobs: 사이킷런이 훈련과 예측에 사용할 cpu 코어 수\n",
        "  - (-1)을 사용하면 모든 코어를 사용한다는 의미"
      ],
      "metadata": {
        "id": "UD8-4mzPPfAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "bag_clf.fit(X_train,y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "aLzdC4g7Jhw8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과\n",
        "      1. 부트스트래핑은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 높음\n",
        "      2. 하지만 다양성을 추가 = 예측기의 상관관계를 줄임\n",
        "        -> 앙상블의 분산을 감소시킴\n",
        "      3. 전반적으로 배깅이 더 나은 모델을 만들기 때문에 일반적으로 더 선호함\n",
        "      4. 시간과 cpu 파워에 여유가 있다면 배깅과 페이스팅을 모두 평가해서 더 좋은 쪽을 선택하는 것이 best"
      ],
      "metadata": {
        "id": "z9oKWWNZNr1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7.2.2 obb 평가**"
      ],
      "metadata": {
        "id": "SyE45uwXOb2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링이 되고, 어떤 것은 전혀 선택되지 않음\n",
        "- 배깅은 기본값으로 중복을 허용해(bootstrap=True) 훈련 세트의 크기만큼 m개 샘플을 선택하게 됨\n",
        "  - 이는 평균적으로 63% 정도만 샘플링된다는 것을 의미함\n",
        "  - 즉, **선택되지 않은 훈련 샘플의 나머지 37%를 obb(out-of-bag) 샘플**이라고 불림 -> 예측기마다 남겨진 샘플은 모두 다름\n",
        "\n",
        "- 예측기가 훈련되는 동안 obb 샘플은 사용하지 않으므로 별도의 검증 세트를 사용하지 않고, obb 샘플을 사용해 평가할 수 있음\n",
        "- 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻음"
      ],
      "metadata": {
        "id": "hqoM8TciOeXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),n_estimators=500,\n",
        "    bootstrap=True,n_jobs=-1,oob_score=True)\n",
        "\n",
        "bag_clf.fit(X_train,y_train)\n",
        "bag_clf.oob_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1_Od_Q6M455",
        "outputId": "e9273dd9-b07a-4027-ce10-6f01987bc029"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- obb 평가 결과: 배깅의 테스트 세트에서 약 89.9%의 정확도를 얻을 것으로 보임"
      ],
      "metadata": {
        "id": "TCf0FKTNQAZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS8q8fYbP3u8",
        "outputId": "685e5abf-22af-493c-ff7d-1eb5f0f229ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.896"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과: 테스트 세트에서 89.6%의 정확도를 얻음 -> 예측과 매우 비슷함!"
      ],
      "metadata": {
        "id": "NO3ZowRzQRtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- oob 샘플에 대한 결정 함수의 값: oob_decision_function_변수에서 확인 가능함\n",
        "  - 결정 함수는 각 훈련 샘플의 클래스 확률을 반환함(기반이 되는 예측기가 predict_proba()메서드를 가지고 있기 때문)"
      ],
      "metadata": {
        "id": "hKPNhRCKQY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVWCp8p8QQEv",
        "outputId": "8107211a-d6a5-457c-da62-5093635e6861"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35643564, 0.64356436],\n",
              "       [0.43023256, 0.56976744],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00515464, 0.99484536],\n",
              "       [0.00568182, 0.99431818],\n",
              "       [0.10555556, 0.89444444],\n",
              "       [0.28476821, 0.71523179],\n",
              "       [0.00578035, 0.99421965],\n",
              "       [0.98469388, 0.01530612],\n",
              "       [0.98387097, 0.01612903],\n",
              "       [0.73770492, 0.26229508],\n",
              "       [0.        , 1.        ],\n",
              "       [0.73333333, 0.26666667],\n",
              "       [0.85057471, 0.14942529],\n",
              "       [0.95024876, 0.04975124],\n",
              "       [0.05952381, 0.94047619],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98930481, 0.01069519],\n",
              "       [0.91954023, 0.08045977],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00555556, 0.99444444],\n",
              "       [0.36040609, 0.63959391],\n",
              "       [0.9044586 , 0.0955414 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98795181, 0.01204819],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99447514, 0.00552486],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.5828877 , 0.4171123 ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.17277487, 0.82722513],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.37765957, 0.62234043],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.24      , 0.76      ],\n",
              "       [0.36021505, 0.63978495],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00555556, 0.99444444],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.9939759 , 0.0060241 ],\n",
              "       [0.86153846, 0.13846154],\n",
              "       [0.96111111, 0.03888889],\n",
              "       [0.96703297, 0.03296703],\n",
              "       [0.        , 1.        ],\n",
              "       [0.06417112, 0.93582888],\n",
              "       [0.98404255, 0.01595745],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00540541, 0.99459459],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98888889, 0.01111111],\n",
              "       [0.83809524, 0.16190476],\n",
              "       [0.39772727, 0.60227273],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.64361702, 0.35638298],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.85279188, 0.14720812],\n",
              "       [1.        , 0.        ],\n",
              "       [0.60240964, 0.39759036],\n",
              "       [0.11363636, 0.88636364],\n",
              "       [0.67032967, 0.32967033],\n",
              "       [0.91256831, 0.08743169],\n",
              "       [0.        , 1.        ],\n",
              "       [0.1878453 , 0.8121547 ],\n",
              "       [0.90163934, 0.09836066],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98924731, 0.01075269],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03333333, 0.96666667],\n",
              "       [0.03076923, 0.96923077],\n",
              "       [0.35449735, 0.64550265],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.86708861, 0.13291139],\n",
              "       [0.00584795, 0.99415205],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.23834197, 0.76165803],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00510204, 0.99489796],\n",
              "       [0.00534759, 0.99465241],\n",
              "       [0.94300518, 0.05699482],\n",
              "       [0.81909548, 0.18090452],\n",
              "       [0.01025641, 0.98974359],\n",
              "       [1.        , 0.        ],\n",
              "       [0.24064171, 0.75935829],\n",
              "       [0.61780105, 0.38219895],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03529412, 0.96470588],\n",
              "       [0.52531646, 0.47468354],\n",
              "       [0.99441341, 0.00558659],\n",
              "       [0.02824859, 0.97175141],\n",
              "       [1.        , 0.        ],\n",
              "       [0.22754491, 0.77245509],\n",
              "       [0.44845361, 0.55154639],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01630435, 0.98369565],\n",
              "       [0.98421053, 0.01578947],\n",
              "       [0.24309392, 0.75690608],\n",
              "       [0.91061453, 0.08938547],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.8       , 0.2       ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01129944, 0.98870056],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99425287, 0.00574713],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00549451, 0.99450549],\n",
              "       [0.93956044, 0.06043956],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01734104, 0.98265896],\n",
              "       [0.22905028, 0.77094972],\n",
              "       [0.97674419, 0.02325581],\n",
              "       [0.30729167, 0.69270833],\n",
              "       [0.98734177, 0.01265823],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.67391304, 0.32608696],\n",
              "       [0.42473118, 0.57526882],\n",
              "       [0.47208122, 0.52791878],\n",
              "       [0.86243386, 0.13756614],\n",
              "       [0.90243902, 0.09756098],\n",
              "       [0.04545455, 0.95454545],\n",
              "       [0.84126984, 0.15873016],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02777778, 0.97222222],\n",
              "       [0.98342541, 0.01657459],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00985222, 0.99014778],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02475248, 0.97524752],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.96531792, 0.03468208],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99435028, 0.00564972],\n",
              "       [0.        , 1.        ],\n",
              "       [0.34554974, 0.65445026],\n",
              "       [0.24858757, 0.75141243],\n",
              "       [0.01086957, 0.98913043],\n",
              "       [0.        , 1.        ],\n",
              "       [0.35828877, 0.64171123],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97765363, 0.02234637],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.6875    , 0.3125    ],\n",
              "       [0.9408284 , 0.0591716 ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.1147541 , 0.8852459 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03589744, 0.96410256],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03243243, 0.96756757],\n",
              "       [0.99479167, 0.00520833],\n",
              "       [0.93785311, 0.06214689],\n",
              "       [0.76262626, 0.23737374],\n",
              "       [0.58152174, 0.41847826],\n",
              "       [0.        , 1.        ],\n",
              "       [0.14444444, 0.85555556],\n",
              "       [1.        , 0.        ],\n",
              "       [0.93401015, 0.06598985],\n",
              "       [0.98742138, 0.01257862],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00502513, 0.99497487],\n",
              "       [0.00564972, 0.99435028],\n",
              "       [0.47311828, 0.52688172],\n",
              "       [0.84916201, 0.15083799],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00552486, 0.99447514],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98224852, 0.01775148],\n",
              "       [0.        , 1.        ],\n",
              "       [0.25123153, 0.74876847],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00571429, 0.99428571],\n",
              "       [0.94705882, 0.05294118],\n",
              "       [0.82291667, 0.17708333],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.1       , 0.9       ],\n",
              "       [0.99479167, 0.00520833],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05113636, 0.94886364],\n",
              "       [1.        , 0.        ],\n",
              "       [0.78021978, 0.21978022],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87719298, 0.12280702],\n",
              "       [0.98395722, 0.01604278],\n",
              "       [0.19565217, 0.80434783],\n",
              "       [0.23834197, 0.76165803],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00531915, 0.99468085],\n",
              "       [0.24870466, 0.75129534],\n",
              "       [0.97701149, 0.02298851],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98907104, 0.01092896],\n",
              "       [0.        , 1.        ],\n",
              "       [0.5       , 0.5       ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00549451, 0.99450549],\n",
              "       [0.00552486, 0.99447514],\n",
              "       [0.06703911, 0.93296089],\n",
              "       [0.11956522, 0.88043478],\n",
              "       [0.98387097, 0.01612903],\n",
              "       [0.02673797, 0.97326203],\n",
              "       [1.        , 0.        ],\n",
              "       [0.32524272, 0.67475728],\n",
              "       [0.06122449, 0.93877551],\n",
              "       [0.50485437, 0.49514563],\n",
              "       [0.64044944, 0.35955056],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.62011173, 0.37988827],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.17613636, 0.82386364],\n",
              "       [0.85869565, 0.14130435],\n",
              "       [0.07291667, 0.92708333],\n",
              "       [1.        , 0.        ],\n",
              "       [0.75555556, 0.24444444],\n",
              "       [0.        , 1.        ],\n",
              "       [0.0052356 , 0.9947644 ],\n",
              "       [0.06321839, 0.93678161],\n",
              "       [0.03092784, 0.96907216],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99456522, 0.00543478],\n",
              "       [0.88950276, 0.11049724],\n",
              "       [0.18857143, 0.81142857],\n",
              "       [0.93048128, 0.06951872],\n",
              "       [0.01775148, 0.98224852],\n",
              "       [0.57608696, 0.42391304],\n",
              "       [0.07      , 0.93      ],\n",
              "       [0.98275862, 0.01724138],\n",
              "       [0.84530387, 0.15469613],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.96923077, 0.03076923],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.25280899, 0.74719101],\n",
              "       [0.98958333, 0.01041667],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00510204, 0.99489796],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87692308, 0.12307692],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.77319588, 0.22680412],\n",
              "       [0.91457286, 0.08542714],\n",
              "       [1.        , 0.        ],\n",
              "       [0.685     , 0.315     ],\n",
              "       [0.51052632, 0.48947368],\n",
              "       [0.        , 1.        ],\n",
              "       [0.91525424, 0.08474576],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.86842105, 0.13157895],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.75      , 0.25      ],\n",
              "       [0.12790698, 0.87209302],\n",
              "       [0.54545455, 0.45454545],\n",
              "       [0.15343915, 0.84656085],\n",
              "       [0.        , 1.        ],\n",
              "       [0.85555556, 0.14444444],\n",
              "       [0.83615819, 0.16384181],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99428571, 0.00571429],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02912621, 0.97087379],\n",
              "       [0.97687861, 0.02312139],\n",
              "       [0.96470588, 0.03529412],\n",
              "       [1.        , 0.        ],\n",
              "       [0.45882353, 0.54117647],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98930481, 0.01069519],\n",
              "       [0.01169591, 0.98830409],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.9895288 , 0.0104712 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.06779661, 0.93220339],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99441341, 0.00558659],\n",
              "       [0.02072539, 0.97927461],\n",
              "       [1.        , 0.        ],\n",
              "       [0.14285714, 0.85714286],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.36842105, 0.63157895],\n",
              "       [0.10769231, 0.89230769],\n",
              "       [0.195     , 0.805     ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.97860963, 0.02139037],\n",
              "       [0.21111111, 0.78888889],\n",
              "       [0.97765363, 0.02234637],\n",
              "       [0.00473934, 0.99526066],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.9673913 , 0.0326087 ],\n",
              "       [0.35869565, 0.64130435],\n",
              "       [0.98369565, 0.01630435],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00578035, 0.99421965],\n",
              "       [0.03488372, 0.96511628],\n",
              "       [0.98963731, 0.01036269],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03030303, 0.96969697],\n",
              "       [0.6684492 , 0.3315508 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.3 랜덤 패치와 랜덤 서브스페이스**\n",
        "---"
      ],
      "metadata": {
        "id": "nNPFRKHIQ1Tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BaggingClassifier는 **특성 샘플링**도 지원함\n",
        "      1. 샘플링은 max_features, bootstrap_features 두 개의 매개변수로 조절됨\n",
        "      2. max_samples와 bootstrap과 동일하지만, 샘플이 아닌 **특성**에 대한 샘플링\n",
        "      3. 각 예측기는 무작위로 선택한 입력 특성의 일부분으로 훈련됨\n",
        "      4. 이 기법은 이미지와 같은 고차원의 데이터셋을 다룰 때 유용함\n",
        "\n",
        "- **랜덤 패치 방식:** 훈련 특성과 샘플을 모두 샘플링하는 방법\n",
        "- **랜덤 서브스페이스 방식:** 훈련 샘플을 모두 사용하고 특성은 샘플링하는 방식\n",
        "  - `bootstrap=False`, `max_samples=1.0`\n",
        "  - `bootstrap_features=True`, `max_samples < 1.0`\n",
        "\n",
        "- 특정 샘플링은 더 다양한 예측기를 만들어 편향을 늘리는 대신 분산을 낮춤"
      ],
      "metadata": {
        "id": "hd81R8tQQ9lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.4 랜덤 포레스트**\n",
        "---"
      ],
      "metadata": {
        "id": "3zrJHuvhT80C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트: 일반적으로 배깅 방법(or 페이스팅)을 적용한 결정 트리의 앙상블\n",
        "  - 전형적으로 **max_samples**를 훈련 세트의 크기로 지정함\n",
        "  - BaggingClassifier에 DecisionTreeClassifier를 넣어 만드는 대신에 결정 트리에 최적화되어 사용하기 편리한 **RandomForestClassifier**를 사용할 수 있음"
      ],
      "metadata": {
        "id": "Erf4CjKQUouy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 최대 16개의 노드를 갖는 500개 세트로 이루어진 랜덤 포레스트 분류기를 여러 cpu 코어에서 훈련시키기"
      ],
      "metadata": {
        "id": "JIArf6DiVUhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)\n",
        "rnd_clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred = rnd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "PI8QD-1eQz0r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트의 특징\n",
        "      1. 의사결정트리의 매개변수와 앙상블 자체를 제어하는데 필요한 BaggingClassifier의 매개변수를 모두 가지고 있음\n",
        "      2. 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 **무작위성을 더 주입함**\n",
        "        -> 이는 트리를 더 다양하게 만들고 다시 편향을 손해보는 대신에 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어냄"
      ],
      "metadata": {
        "id": "t3PfQlYUVxbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BaggingClassifier을 사용해 랜덤 포레스트와 유사하게 만들기\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(max_features='auto',max_leaf_nodes=16),\n",
        "    n_estimators=500,max_samples=1.0,bootstrap=True,n_jobs=-1)"
      ],
      "metadata": {
        "id": "KtjQa1RoVmAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7.4.1 엑스트라 트리**"
      ],
      "metadata": {
        "id": "IbonLQtXW65u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트에서 트리를 만들 때 각 노드는 무작위로 특성의 서브셋을 만들어 분할에 사용됨\n",
        "- 트리를 더 무작위성을 부여하기 위해 보통의 결정 트리처럼 최적의 임계값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그 중에서 최상의 분할을 선택함\n",
        "- **익스트림 랜덤 트리 앙상블:** 극단적으로 무작위한 트리의 랜덤 포레스트(extra-tree)\n",
        "  - 편향 up, 분산 down\n",
        "  - ExtraTreesClassifier  \n",
        "\n",
        "- 모든 노드에서 특성마다 가장 최적의 임계값을 찾는 것이 트리 알고리즘에서 가장 많은 시간이 소요되므로 일반적인 랜덤 포레스트보다 엑스트라 트리가 훨씬 빠름"
      ],
      "metadata": {
        "id": "m0SNJfiEW9Va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7.4.2 특성 중요도**"
      ],
      "metadata": {
        "id": "GbO0YAjpY-1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트의 장점: 특성의 상대적 중요도를 측정하기 쉬움\n",
        "- 사이킷런은 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정함-> `가중치 평균` (`가중치`=연관된 훈련 샘플 수)\n",
        "- 사이킷런은 훈련이 끝난 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결과값을 정규화함\n",
        "  - `feature_importances_`변수에 값이 저장되어 있음"
      ],
      "metadata": {
        "id": "jfsRiap9ZDii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
        "rnd_clf.fit(iris['data'],iris['target'])\n",
        "for name,score in zip(iris['feature_names'],rnd_clf.feature_importances_):\n",
        "  print(name,score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT0f_mWBY9Ys",
        "outputId": "3d7e2a33-c70a-40d7-d06a-f37596c0d494"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm) 0.09591694066352541\n",
            "sepal width (cm) 0.022371024817084376\n",
            "petal length (cm) 0.41637307015804964\n",
            "petal width (cm) 0.46533896436134065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트는 특성을 선택할 때 어떤 특성이 중요한지 빠르게 확인할 수 있어 매우 편리함"
      ],
      "metadata": {
        "id": "VmALEOTea6AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.5 부스팅**\n",
        "---"
      ],
      "metadata": {
        "id": "s6vyPbgYa_If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 부스팅: 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
        "  - 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것\n",
        "  - **adaboost**\n",
        "  - **gradient boosting**"
      ],
      "metadata": {
        "id": "e-YLrGVTbB42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7.5.1 에이다부스트**"
      ],
      "metadata": {
        "id": "eB8QLr_0bvm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특징\n",
        "      1. 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높여야 이전 예측기를 보완하는 새로운 예측기를 생성함\n",
        "      2. 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨"
      ],
      "metadata": {
        "id": "50PQH5TSbzFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 에이다부스트 알고리즘\n",
        "  - 각 샘플 가중치 $w^{(i)}$를 $\\frac{1}{m}$으로 초기화하고, 첫 번째 모델을 학습함\n",
        "  - 가중치가 적용된 에러율 $r_1$이 훈련 세트에 대해 계산됨 $$r_j = \\frac{\\sum_{i=1, \\hat{y_j} \\ne y^{(i)}}^{m}w^{(i)}}{\\sum_{i=1}^{m} w^{(i)}}$$\n",
        "\n",
        "  - 예측 모델의 가중치 $\\alpha_j$를 계산함 -> 여기서 $\\eta$는 **학습률 하이퍼 파라미터**로, 예측 모델이 정확할수록 가중치가 더 높아짐\n",
        "    $$\\alpha_j= \\eta log{\\frac{1-r_j}{r_j}}$$\n",
        "  - 샘플의 가중치를 업데이트함 -> 잘못 분류된 샘플의 가중치가 증가됨\n",
        "  $$ w^{(i)} = \n",
        "  \\begin{cases}\n",
        "  w^{(i)}, & \\mbox{if }\\hat{y_j} = y^{(i)} \\\\\n",
        "  w^{(i)} exp(\\alpha_j), & \\mbox{if }\\hat{y_j} \\ne y^{(i)}\n",
        "  \\end{cases} $$\n",
        "  - 모든 샘플의 가중치를 정규화함\n",
        "  - 새 예측기가 업데이트된 가중치를 사용해 훈련되고 위 과정이 반복됨\n",
        "  - 지정된 예측기 수에 도달하거나 완벽한 예측기가 만들어지면 중지됨\n",
        "  - 예측을 할 때 단순히 모든 예측기의 예측을 계산하고 예측기 가중치 $\\alpha_j$를 더해 예측 결과를 만듦 -> 가중치 합이 가장 큰 클래스가 예측 결과가 됨\n",
        "\n",
        "- 사이킷런은 **SAMME**라는 에이다부스트의 다중 클래스 버전을 사용함\n",
        "- 클래스가 2개일 경우엔 에이다부스트와 SAMME가 동일함\n",
        "- 예측기가 클래스 확률을 측정할 수 있다면(=predict_proba()메서드가 있다면) 사이킷런은 SAMME.R(REAL)이라는 변종을 사용함 -> 예측값 대신에 클래스 확률에 기반하여 일반적으로 성능이 더 좋음"
      ],
      "metadata": {
        "id": "FFVK4FkJdyFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷런의 AdaBoostClassifier 사용하기\n",
        "- 200개의 아주 얕은 결정 트리를 기반으로 하는 에이다부스트 분류기를 훈련시키기\n",
        "- 결정트리: max_depth=1(결정 노드 1개와 리프 노드 2개로 이루어진 트리)"
      ],
      "metadata": {
        "id": "AuQpRmsRhlRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),n_estimators=200,\n",
        "    algorithm='SAMME.R',learning_rate=0.5)\n",
        "ada_clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcS_ZkdxY9Wa",
        "outputId": "8eceeb47-4e50-4a73-9c41-34dc4f2756ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7.5.2 그레이디언트 부스팅**"
      ],
      "metadata": {
        "id": "eIRjOLYJiidg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가함\n",
        "- 에이다부스트와의 차이점: \n",
        "  - **에이다부스트:** 반복마다 샘플의 가중치를 수정하면서 학습\n",
        "  - **그래디언트 부스팅:** 이전 예측기가 만든 **잔여 오차**에 새로운 예측기를 학습"
      ],
      "metadata": {
        "id": "HDUx0bj-ioNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **그레이디언트 트리 부스팅(그레이디언트 부스티드 회귀 트리)**"
      ],
      "metadata": {
        "id": "ihA0qw6hjRp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data 다시 만들기\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
      ],
      "metadata": {
        "id": "2tt-5mQijs6T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LapV71vQY9UK",
        "outputId": "ba1a797f-868b-4634-fe07-1461d869fcd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 첫번째 예측기에서 생긴 잔여 오차에 두번째 DecisionTreeRegressor를 훈련시키기"
      ],
      "metadata": {
        "id": "94b8m5QYj3wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X,y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0jzEIbAjf6C",
        "outputId": "d467c644-2135-4252-b48b-5b9f5f907eb0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X,y3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWGYoituY9Bj",
        "outputId": "853bd729-2a3c-42da-ea90-6ce88ca1417b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 새로운 샘플에 대한 예측을 만들기 위해 모든 트리의 예측 더하기"
      ],
      "metadata": {
        "id": "FoSiYKXskNcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 세트 생성하기\n",
        "X_new = np.array([[0.8]])"
      ],
      "metadata": {
        "id": "j5I3sQc9ken5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1,tree_reg2,tree_reg3))"
      ],
      "metadata": {
        "id": "X9F9kvYkkRPs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷런의 GradientBoostingRegressor로 GBRT 앙상블을 간단하게 훈련시킬 수 있음\n",
        "  - n_estimators\n",
        "  - max_depth\n",
        "  - min_samples_leaf"
      ],
      "metadata": {
        "id": "rp8o9gLrk4Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)\n",
        "gbrt.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p1u1GGvkRpG",
        "outputId": "bf518903-ff98-4223-b483-2dd628705028"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- staged_predict(): 최적의 트리 수를 찾기 위해서 사용하는 조기 종료 기법\n",
        "  - 훈련의 각 단계에서 앙상블에 의해 만들어진 예측기를 순회하는 반복자(iterator)를 반환함"
      ],
      "metadata": {
        "id": "WimNNGFllm6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train,X_val,y_train,y_val = train_test_split(X,y)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=120)\n",
        "gbrt.fit(X_train,y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1 # argmin: 최소값인 인덱스 찾기\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azZ5_u85kRnA",
        "outputId": "4729d5d2-1386-4ee6-859b-591f43effc7b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(max_depth=2, n_estimators=60)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **warm_start=True:** 훈련을 중지하는 방법으로 조기 종료를 구현하는 방법. fit()메서드 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있게 함"
      ],
      "metadata": {
        "id": "UtG_IucUopFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2,warm_start=True)\n",
        "\n",
        "min_val_error = float('inf')\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1,120):\n",
        "  gbrt.n_estimators = n_estimators\n",
        "  gbrt.fit(X_train,y_train)\n",
        "  y_pred = gbrt.predict(X_val)\n",
        "  val_error = mean_squared_error(y_val,y_pred)\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    error_going_up = 0\n",
        "  else:\n",
        "    error_going_up += 1\n",
        "    if error_going_up == 5:\n",
        "      break"
      ],
      "metadata": {
        "id": "059xrIJTkRkd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **확률적 그레이디언트 부스팅:** 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 정할 수 있는 `sumsample` 매개변수도 지원함\n",
        "  - 편향 up, 분산down\n",
        "  - 훈련 속도 up\n",
        "\n",
        "- XGBoost: 최적화된 그레이디언트 부스팅"
      ],
      "metadata": {
        "id": "L8BGFC4ZpeG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train,y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653gvhb9kRiT",
        "outputId": "52138e3a-b067-4660-b0eb-04ff0432e840"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17:18:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **자동 조기 종료**"
      ],
      "metadata": {
        "id": "h_cWIdgvqYup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_reg.fit(X_train,y_train,\n",
        "            eval_set=[(X_val,y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_eRsygkqC6c",
        "outputId": "14ddcf22-cd38-4d9e-8157-ae274e5eb88d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17:19:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:0.280663\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:0.252268\n",
            "[2]\tvalidation_0-rmse:0.226486\n",
            "[3]\tvalidation_0-rmse:0.203519\n",
            "[4]\tvalidation_0-rmse:0.183563\n",
            "[5]\tvalidation_0-rmse:0.165742\n",
            "[6]\tvalidation_0-rmse:0.15084\n",
            "[7]\tvalidation_0-rmse:0.137032\n",
            "[8]\tvalidation_0-rmse:0.126024\n",
            "[9]\tvalidation_0-rmse:0.115642\n",
            "[10]\tvalidation_0-rmse:0.105905\n",
            "[11]\tvalidation_0-rmse:0.098178\n",
            "[12]\tvalidation_0-rmse:0.091328\n",
            "[13]\tvalidation_0-rmse:0.085401\n",
            "[14]\tvalidation_0-rmse:0.080359\n",
            "[15]\tvalidation_0-rmse:0.076347\n",
            "[16]\tvalidation_0-rmse:0.07327\n",
            "[17]\tvalidation_0-rmse:0.070553\n",
            "[18]\tvalidation_0-rmse:0.068311\n",
            "[19]\tvalidation_0-rmse:0.066384\n",
            "[20]\tvalidation_0-rmse:0.064861\n",
            "[21]\tvalidation_0-rmse:0.063184\n",
            "[22]\tvalidation_0-rmse:0.062026\n",
            "[23]\tvalidation_0-rmse:0.060499\n",
            "[24]\tvalidation_0-rmse:0.059329\n",
            "[25]\tvalidation_0-rmse:0.058682\n",
            "[26]\tvalidation_0-rmse:0.057698\n",
            "[27]\tvalidation_0-rmse:0.056953\n",
            "[28]\tvalidation_0-rmse:0.056407\n",
            "[29]\tvalidation_0-rmse:0.055397\n",
            "[30]\tvalidation_0-rmse:0.054512\n",
            "[31]\tvalidation_0-rmse:0.053824\n",
            "[32]\tvalidation_0-rmse:0.053183\n",
            "[33]\tvalidation_0-rmse:0.053253\n",
            "[34]\tvalidation_0-rmse:0.052814\n",
            "[35]\tvalidation_0-rmse:0.052414\n",
            "[36]\tvalidation_0-rmse:0.052011\n",
            "[37]\tvalidation_0-rmse:0.051775\n",
            "[38]\tvalidation_0-rmse:0.05182\n",
            "[39]\tvalidation_0-rmse:0.051829\n",
            "Stopping. Best iteration:\n",
            "[37]\tvalidation_0-rmse:0.051775\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.6 스태킹**\n",
        "---"
      ],
      "metadata": {
        "id": "sw3nGA41qc_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앙상블에 속한 모든 예측기의 예측을 취합하는 모델\n",
        "- **블렌더 or 메타 학습기:** 마지막 예측기로, 각 모델의 예측값을 입력받아 최종 예측을 함\n",
        "- **홀드 아웃 세트**\n",
        "      1. 훈련 세트를 두 개의 서브셋으로 나눔\n",
        "      2. 첫 번째 서브셋: 첫 번째 레이어의 예측을 훈련하기 위해 사용됨\n",
        "      3. 첫 번째 레이어의 예측기를 사용해 두 번째(홀드아웃) 세트에 대한 예측을 만듦\n",
        "        -> 예측기들이 훈련하는 동안 이 샘플들을 보지 못했기 때문에 이때 만들어진 예측은 완전히 새로운 것\n",
        "      4. 타깃값은 그대로 쓰고 앞에서 예측한 값을 입력 특성으로 사용하는 새로운 훈련 세트를 만들 수 있음\n",
        "      5. 블렌더를 새 훈련 세트로 훈련함"
      ],
      "metadata": {
        "id": "YbHwoRkhqfH5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpNcDuGsqP17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
